.. Kithara documentation master file, created by
   sphinx-quickstart on Wed Nov 20 10:35:12 2024.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

ğŸ‘‹ Welcome to Kithara!
===================================
.. note::

   This project is under active development.

Introduction
------------

What is Kithara?
~~~~~~~~~~~~~~~~

**Kithara** is a lightweight library offering building blocks and recipes for tuning popular open source LLMs like Llama 3 and Gemma 2 on Google TPUs.

Kithara makes post-training easier to do on TPUs.  Post-training algorithms include continued pretraining, fine-tuning, PEFT, RLHF, and quantization. These algorithms adapt a pre-trained language model to a specific task or domain by training it on a custom dataset. Using TPUs provides significant advantages in terms of performance, cost-effectiveness, and scalability, enabling faster training times and the ability to work with larger models and datasets.

.. grid:: 1
    :gutter: 3
    :margin: 0
    :class-container: full-width g-0

    .. grid-item-card:: Choose Kithara for the following benefits:
        :padding: 1
        :class-item: g-0

        â†’ Hugging Face checkpoints accepted and generated by default
        
        â†’ Simple pip install
        
        â†’ User-friendly documentation
        
        â†’ Library-based UX
        
        â†’ Wide variety of models through Keras Hub
        
        â†’ Performant models through Maxtext
        
        â†’ Custom trainer code in Keras or Jax
        
        â†’ Ray data loading
        
        â†’ Version Control
        
        â†’ Reuse code on GPUs
        
        â†’ Wide variety of tuning algorithms:  SFT, LoRA, Instruction tuning.
        
        â†’ Coming soon: RL (DPO, PPO, ORPO, GRPO), Distillation, Quantization
        
        â†’ Coming soon: Ray Tune for automatic hyperparameter selection


Target Audience
~~~~~~~~~~~~~~~

This guide is intended for developers and researchers familiar with machine learning and large language models who want to leverage the power of TPUs for fine-tuning and other post-training workloads. It is structured to guide you through the process, from setup to deployment, with practical examples and explanations of key concepts.

Key Features
~~~~~~~~~~~~

This product supports a variety of tuning algorithms (SFT, DPO, LoRA, Continued pre-training), data formats (JSONL, Parquet, CSV, Text, HuggingFace Dataset, and more), and models (including Gemma 2 2B, 9B, 27B; Llama 3.1 8B, 70B). It also includes performance optimizations (FlashAttention, Scanning, Rematerialization) and parallelism options (FSDP, FSDP+DDP).

Design Principles
~~~~~~~~~~~~~~~~~~
Easy to use
    â†’ Frictionless onboarding

Hardware Flexibility
    â†’ Use the same code on GPU and TPU 

Developer Centric
    â†’ Designed for customization instead of black-box recipes


Get Started
-----------
.. grid:: 2
    :gutter: 3
    :margin: 0
    :class-container: full-width g-0

    .. grid-item-card:: ğŸ›’ Getting TPUs
        :link: getting_tpus
        :link-type: ref
        :columns: 4
        :padding: 2
        :class-item: g-0

        `New to TPUs? Here is a guide for determining which TPUs to get and how to get them.`

    .. grid-item-card:: âš’ï¸ Installation
        :link: installation
        :link-type: ref
        :columns: 4
        :padding: 2
        :class-item: g-0

        `Quick PiP installation guide.`

    .. grid-item-card:: âœï¸ Quickstart
        :link: quickstart
        :link-type: ref
        :columns: 4
        :padding: 2
        :class-item: g-0

        `Fine-tune a Gemma2 2B model using LoRA.`

    .. grid-item-card:: ğŸ“ Finetuning Guide
        :link: finetuning_guide
        :link-type: ref
        :columns: 4
        :padding: 2
        :class-item: g-0

        `Step-by-step guide for finetuning your model.`

    .. grid-item-card:: ğŸ“ˆ Scaling up with Ray
        :link: scaling_with_ray
        :link-type: ref
        :columns: 4
        :padding: 2
        :class-item: g-0

        `Guide for running multihost training with Ray.`

    .. grid-item-card:: ğŸ“– API documentation
        :link: model_api
        :link-type: ref
        :columns: 4
        :padding: 2
        :class-item: g-0

        `API documentation for Kithara library components.`
        
.. toctree::
   :caption: Welcome
   :hidden:


   ğŸ›’ Getting TPUs <getting_tpus>
   âš’ï¸ Installation <installation>
   âœï¸ Quickstart <quickstart>
   ğŸ“ Supported Models <models>
   ğŸ“– Supported Data Formats <datasets>
   ğŸ“ Finetuning Guide <finetuning_guide>
   ğŸ“ˆ Scaling up with Ray <scaling_with_ray>
   ğŸ Serve with vLLM <vllm>
   ğŸ’¡ Troubleshooting <troubleshooting>
   ğŸ’Œ Support and Community <support>

.. toctree::
   :caption: Basics
   :hidden:

   ğŸŒµ SFT Example <sft>
   ğŸŒµ Continued Pretraining Example <pretraining>
   âœ¨ LoRA <lora>
   ğŸ“¦ Dataset Packing <packing>
   ğŸ“š Managing Large Datasets <ddp>
   ğŸ” Observability <observability>
   ğŸ”– Checkpointing <checkpointing>
   ğŸš€ Performance Optimizations <optimizations>

.. toctree::
   :caption: API
   :hidden:

   Model <api/kithara.model_api>
   Dataset <api/kithara.dataset_api>
   Trainer <api/kithara.trainer_api>

