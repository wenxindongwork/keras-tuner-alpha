# This example shows how to launch a task using a config based interface. 
# If you prefer writing out your own training script, check out sft_lora_example.py.

# Launch this example using the following command:
# python kithara/config/launcher.py --override_config=examples/singlehost/sft_lora_example.yaml

# -------------------------
# Model Configuration
# -------------------------
model_handle: "hf://google/gemma-2-2b"
tokenizer_handle: "hf://google/gemma-2-2b"
seq_len: 4096  
use_lora: True  
lora_r: 16

# -------------------------
# Training Settings
# -------------------------
training_steps: 100   
eval_steps_interval: 20   
log_steps_interval: 10  

# -------------------------
# Checkpointing
# -------------------------
save_checkpoints: False

# -------------------------
# Train Dataset
# -------------------------
train_dataset_type: "huggingface"  # Currently only huggingface is supported in the config-based interface
train_hf_dataset_path: "yahma/alpaca-cleaned"
train_dataset_column_mapping: {"prompt":"input", "answer":"output"}   
eval_dataset_column_mapping: {"prompt":"input", "answer":"output"}   
train_eval_split: 50 # Use 50 test samples
train_sample_packing: False 
eval_sample_packing: False 

# -------------------------
# Output Configuration
# -------------------------
save_model: true  # Whether to save the final model
only_save_adapters: true  # Only save LoRA adapters (ignored if use_lora=false)
model_output_dir: "model_output/"