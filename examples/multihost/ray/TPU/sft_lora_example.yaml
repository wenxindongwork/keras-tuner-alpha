# Launch this example using the following command:

# Replace v5e with your tpu generation 
# python ray/submit_job.py "python3.11 kithara/config/launcher.py --override_config=examples/multihost/sft_lora_example.yaml --tpu_generation=v5e" --hf-token your_token

# -------------------------
# Orchestration Configuration
# -------------------------
single_host: False

# -------------------------
# Model Configuration
# -------------------------
seq_len: 8192  
model_handle: "hf://google/gemma-2-9b"

# -------------------------
# Training Settings
# -------------------------
training_steps: 100   
eval_steps_interval: 20   
log_steps_interval: 10  

# -------------------------
# Checkpointing
# -------------------------
save_checkpoint_interval: 20   
max_checkpoints_to_keep: 3   
checkpoint_dir: "gs://wenxindong-tpu-prod-env-multipod-bucket/checkpoints/" 

# -------------------------
# Logging
# -------------------------
tensorboard_dir: "gs://wenxindong-tpu-prod-env-multipod-bucket/tensorboard_dir/"

# -------------------------
# Train Dataset
# -------------------------
train_dataset_column_mapping: {"prompt":"input", "answer":"output"}   
train_eval_split: 50 # Use 50 test samples

# -------------------------
# Output Configuration
# -------------------------
model_output_dir: "gs://wenxindong-tpu-prod-env-multipod-bucket/model_output/"