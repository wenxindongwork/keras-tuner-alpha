{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659a56d7-fc62-40df-b43e-965c5ae7114f",
   "metadata": {},
   "source": [
    "# *Kithara* - Finetune LLMs on TPU and GPU\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Introduction to Kithara\n",
    "2. Kithara Demo\n",
    "\n",
    "## Introduction to Kithara\n",
    "\n",
    "Kithara will be an accelerator-agnostic, lightweight library offering tools and recipes for tuning popular open source LLMs on TPUs and GPUs. \n",
    "\n",
    "go/kithara-dd\n",
    "\n",
    "go/kithara-slides  \n",
    "\n",
    "go/kithara-design-review-recording \n",
    "\n",
    "## Kithara Demo \n",
    "\n",
    "The goal of this demo is to show the following key features of Kithara. \n",
    "\n",
    "1. Native integration with HuggingFace: Load and save models in HuggingFace format\n",
    "\n",
    "2. LoRA support\n",
    "\n",
    "3. Ease of scaling single host workload to multihost\n",
    "\n",
    "4. GPU/TPU Fungibility - same code runs on both GPU and TPUs\n",
    "\n",
    "5. Extensive dataset format support\n",
    "\n",
    "6. Smart defaults for model optimizations and parallelism\n",
    "\n",
    "7. Support for tuning MaxText models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca56d1b7-1c2a-4a95-b88e-f00f3247b8ab",
   "metadata": {},
   "source": [
    "### This demo is currently running on V4-8 (single-host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7154bdfd-1625-4f5c-a9b9-3369e64d6c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: ['tpu:0', 'tpu:1', 'tpu:2', 'tpu:3']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import keras\n",
    "devices = keras.distribution.list_devices()\n",
    "print(f\"Available devices: {devices}\")\n",
    "\n",
    "import jax \n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/dev/shm/temp/xla_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0fc1e7",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f28cd7b-f874-4764-ad10-6d6684490a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-04 09:16:09,979\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-12-04 09:16:11,186\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import keras_tuner as kithara"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8ecbc-6f08-4483-bc5a-c2986263a374",
   "metadata": {},
   "source": [
    "### Load model from HuggingFace Hub, Enable LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3365ac1-d675-43af-a88c-456c3a7f30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handle = \"hf://google/gemma-2-2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2509e6-dfde-4e7e-8bac-acfcc231b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kithara.KerasHubModel(\n",
    "    model_handle=model_handle,\n",
    "    precision=\"mixed_bfloat16\",\n",
    "    lora_rank=4,\n",
    "    # Predefined Sharding Strategy \n",
    "    sharding_strategy=kithara.ShardingStrategy(\n",
    "        parallelism=\"fsdp\", model=\"gemma\"\n",
    "    ),\n",
    "    # Flash Attention is activated by default\n",
    "    use_flash_attention= True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6aa536-d5da-4df8-8e56-2a8da718654a",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "Kithara will support an extensive list of datasets and dataset formats, including HuggingFace, CSV, JSON, JSONL, and more. \n",
    "\n",
    "*Features:* \n",
    "- Streaming Dataset\n",
    "- Multihost distributed dataloading\n",
    "- Integration with Cloud: GCS, Azure, AWS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa3f1e-fa9c-49f5-a6d3-2b8481e8b5d7",
   "metadata": {},
   "source": [
    "#### Data source: HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e621ef8-66fd-4c3b-a0db-1156c4cef8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the C4 dataset from HuggingFace. Load in streaming mode. \"\"\"\n",
    "from datasets import load_dataset\n",
    "train_ds = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "test_ds = load_dataset(\"allenai/c4\", \"en\", split=\"validation\", streaming=True)\n",
    "train_ds, test_ds = ray.data.from_huggingface(train_ds), ray.data.from_huggingface(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f46bd63-d961-4ffc-96c7-c434781a3732",
   "metadata": {},
   "source": [
    "#### Data source: CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6c4df-e741-49a6-a86f-c3f22726f1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load CSV dataset from Cloud\"\"\"\n",
    "ds = ray.data.read_csv(\"s3://anonymous@ray-example-data/iris.csv\")\n",
    "train_ds, test_ds = ds.train_test_split(test_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2688a94-a108-46fa-a7b3-a7e66edabdb2",
   "metadata": {},
   "source": [
    "#### Data source: TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62fa869-fe69-4435-9209-7c77f49b25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load TFRecords from Cloud\"\"\"\n",
    "ds = ray.data.read_tfrecords(\"s3://anonymous@ray-example-data/iris.tfrecords\")\n",
    "train_ds, test_ds = ds.train_test_split(test_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750097c-3032-41d7-b328-f6ab569609dc",
   "metadata": {},
   "source": [
    "#### Data source: Python Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b6cfc-c7e0-4d00-b269-fae142053b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a toy dataset using Python Dictionary for demo.\"\"\"\n",
    "\n",
    "\"\"\"We want to teach the model to answer a question with a specific response.\"\"\"\n",
    "dataset_items = [\n",
    "    {\n",
    "    \"prompt\": \"What is your name?\",\n",
    "    \"answer\": \"My name is Kithara\"\n",
    "    } \n",
    "]* 1000\n",
    "\n",
    "ds = ray.data.from_items(dataset_items)\n",
    "train_ds, test_ds = ds.train_test_split(test_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26da98-5367-4d1b-a1a7-ea6d7233289b",
   "metadata": {},
   "source": [
    "#### Other supported data formats include JSON, Parquet, BigQuery, MongoDB, Spark, TFDS, Torch.data and more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376149d1-4f23-4a27-b9f1-2a64a7f3dd29",
   "metadata": {},
   "source": [
    " ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ab494-909d-4ebd-8e55-76ca25489c06",
   "metadata": {},
   "source": [
    "### Preprocess Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb97e714-dff6-4413-81bc-1746a21c7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates preprocessor\n",
    "preprocessor = kithara.preprocessor.SFTPreprocessor(\n",
    "    tokenizer_handle=model_handle, seq_len=2048\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = kithara.Dataloader(\n",
    "    train_ds,\n",
    "    per_device_batch_size=1,\n",
    ")\n",
    "eval_dataloader = kithara.Dataloader(\n",
    "    test_ds,\n",
    "    per_device_batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774277af",
   "metadata": {},
   "source": [
    "### Run SFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb6a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /tmp/demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56264376-87a5-4c22-a933-33b30c6fb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SFT trainer\n",
    "trainer = kithara.SFTTrainer(\n",
    "    model=model,\n",
    "    preprocessor=preprocessor,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    steps=150,\n",
    "    eval_steps_interval=50,\n",
    "    log_steps_interval=1,\n",
    "    tensorboard_dir=\"/tmp/demo\",\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=5e-5, weight_decay=0.01),\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Test after tuning\n",
    "pred = trainer.generate(\"What is your name?\")\n",
    "print(\"Tuned model generates:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3222a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch tensorboard\n",
    "tensorboard --logdir=/tmp/demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f9a934",
   "metadata": {},
   "source": [
    "#### Save model in HuggingFace format\n",
    "\n",
    "Note currently this feature is not fully designed out yet, we rely on CLI for model conversion for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29530a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Saves the model weights\n",
    "trainer.model.save_weights(\"/dev/shm/temp/tuned_gemma2_2b.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2dbc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Converts the model weights to HF format\n",
    "python keras-hub/tools/gemma/export_gemma_to_hf.py \\\n",
    "  --weights_file /dev/shm/temp/tuned_gemma2_2b.weights.h5 \\\n",
    "  --gemma_version 2 \\\n",
    "  --size 2b \\\n",
    "  --output_dir /dev/shm/temp/tuned_model_in_hf_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18c52c",
   "metadata": {},
   "source": [
    "#### Load model in HF, or vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a63394c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21de0ec5261b4135aa041cb59307951c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading checkpoint into a HuggingFace model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/dev/shm/temp/tuned_model_in_hf_format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint to a vLLM server \n",
    "\n",
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(model=\"/dev/shm/temp/tuned_model_in_hf_format\")  # Load checkpoint tuned with Kithara\n",
    "output = llm.generate(\"What is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9bc4b",
   "metadata": {},
   "source": [
    "#### Tune a MaxText model\n",
    "\n",
    "MaxText model implementations offer the best in class performance on TPUs. Kithara support tuning models available in MaxText. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kithara.MaxTextModel(\n",
    "    model_name=\"gemma2-9b\",\n",
    "    seq_len=4096,\n",
    "    per_device_batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a269f",
   "metadata": {},
   "source": [
    "### Multihost Example\n",
    "\n",
    "We have seen in the previous section how to tune a Gemma2-2b model with LoRA on a v4-8 (singlehost) machine. In this section, we show how we can scale up and tune a Gemma2-9b model with LoRA on a v4-32 (multihost) machine. \n",
    "\n",
    "Kithara offers an orchestration layer via Ray, which works with resources from GCE, GKE, XPK, QRs. \n",
    "\n",
    "It's worth noting that the core Kithara library can work with any orchestrator options. We offer the Ray orchestration abstraction layer for users who are not familiar with multihost development. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63784251",
   "metadata": {},
   "source": [
    "#### Step 1: Launch a Ray Cluster\n",
    "\n",
    "A Ray Cluster is a group of machines, including CPUs, TPUs, and GPUs, that has a host machine and worker machines. The host machine is responsible for scheduling jobs onto worker machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945aece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch a TPU cluster with the provided YAML file with CLI. \n",
    "ray up -y orchestration/multihost/ray/TPU/cluster.yaml\n",
    "\n",
    "# Or, Launch a GPU cluster \n",
    "ray up -y orchestration/multihost/ray/TPU/cluster.yaml\n",
    "\n",
    "# You can also launch a cluster with both TPUs and GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e84cf",
   "metadata": {},
   "source": [
    "#### Step 2: Launch the Ray dashboard\n",
    "\n",
    "You should see a link for opening up the Ray Dashboard on your localhost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray dashboard orchestration/multihost/ray/TPU/cluster.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1c725",
   "metadata": {},
   "source": [
    "#### Submit a multihost job on the TPU Ray cluster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe89325",
   "metadata": {},
   "outputs": [],
   "source": [
    "python orchestration/multihost/ray/submit_ray_job.py \"python examples/multihost/ray/TPU/hf_sft_gemma_example_via_ray.py\" --hf-token your_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c5b64",
   "metadata": {},
   "source": [
    "#### Submit a multihost job on the GPU Ray Cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "python orchestration/multihost/ray/submit_ray_job.py \"python examples/multihost/ray/GPU/hf_gemma_example_via_ray.py\" --hf-token your_token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
